{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Runtime Question Answering with MobileBert model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the end-to-end steps to obtain from a HuggingFace model, convert to ONNX format and then add pre/post processing steps to the ONNX model using onnxruntime-extensions library. And apply directly in a sample mobile android/ios application if applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to pip install `onnxruntime onnx onnxruntime_extensions transformers` as the necessary libraries.\n",
    "\n",
    "```sh\n",
    "    pip install onnx\n",
    "```\n",
    "```\n",
    "    pip install onnxruntime\n",
    "```\n",
    "```\n",
    "    pip install onnxruntime_extensions\n",
    "```\n",
    "```\n",
    "    pip install transformers\n",
    "```\n",
    "\n",
    "To work with Python in Jupyter Notebooks, you must activate an [Anaconda](https://www.anaconda.com/) environment or another Python environment in which you've installed the [Jupyter package](https://pypi.org/project/jupyter/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Prepare ONNX Model from HuggingFace MobileBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onnx_model_from_huggingface(hf_model_name, onnx_model_path):\n",
    "    \"\"\"\n",
    "        Load the model from huggingface and export it to onnx\n",
    "    \"\"\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_name)\n",
    "    model = transformers.MobileBertForQuestionAnswering.from_pretrained(hf_model_name)\n",
    "    \n",
    "    model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=\"question-answering\")\n",
    "    onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "    onnx_inputs, onnx_outputs = transformers.onnx.export(tokenizer, # pretrained generic tokenizer class for the model\n",
    "                                                         model, # pretrained hf model\n",
    "                                                         onnx_config, # onnx configurations which includes input/output names/types info\n",
    "                                                         16, # opset_version - the ONNX version to export the model to\n",
    "                                                         onnx_model_path) # where to save the exported onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ONNX model from huggingface model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/models/mobilebert/modeling_mobilebert.py:547: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(1000),\n",
      "/usr/local/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.11/site-packages/torch/onnx/utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.11/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = Path('mobilebert_uncased_squad_v2.onnx')\n",
    "if not onnx_model_path.exists():\n",
    "    print(\"Creating ONNX model from huggingface model...\")\n",
    "    create_onnx_model_from_huggingface('csarron/mobilebert-uncased-squad-v2', onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output ONNX model is exported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert onnx_model_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model_path: Path):\n",
    "    \"\"\"\n",
    "        Quantize the model, so that it can be run on mobile devices with smaller memory footprint\n",
    "    \"\"\"\n",
    "    quantized_model_path = model_path.with_name(model_path.stem+\"_quant\").with_suffix(model_path.suffix)\n",
    "    quantize_dynamic(model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "    model_path.unlink()\n",
    "    return quantized_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.12/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.12/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.13/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.13/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.14/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.14/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.15/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.15/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.16/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.16/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.17/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.17/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.18/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.18/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.19/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.19/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.20/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.20/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.21/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.21/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.22/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.22/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.23/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.23/attention/self/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Add pre and post processing steps to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions.tools.pre_post_processing import *\n",
    "from onnxruntime_extensions.tools import add_pre_post_processing_to_model as add_ppp\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pre_post_processing(input_model_path: Path, output_model_path: str, model_name: str = \"csarron/mobilebert-uncased-squad-v2\"):\n",
    "    \"\"\"\n",
    "    Add pre and post processing to the model, for tokenization and post processing\n",
    "    \"\"\"\n",
    "    onnx_opset = 16\n",
    "    model = onnx.load(str(input_model_path.resolve(strict=True)))\n",
    "    inputs = [create_named_value(\"input_text\", onnx.TensorProto.STRING, [1, \"num_sentences\"])]  # Fix the batch size to be 1\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    @contextmanager\n",
    "    def temp_vocab_file():\n",
    "        vocab_file = Path.cwd()/ \"vocab.txt\"\n",
    "        yield vocab_file\n",
    "\n",
    "    with temp_vocab_file() as vocab_file:\n",
    "        import json\n",
    "        with open(str(vocab_file), 'w') as f:\n",
    "            f.write(json.dumps(tokenizer.vocab))\n",
    "\n",
    "        pipeline = PrePostProcessor(inputs, onnx_opset)\n",
    "        \n",
    "        tokenizer_args = TokenizerParam(\n",
    "            vocab_or_file=vocab_file,\n",
    "            do_lower_case=True,\n",
    "            tweaked_bos_id=0,\n",
    "            is_sentence_pair=True,\n",
    "        )\n",
    "        \n",
    "        pipeline.add_pre_processing(\n",
    "            [\n",
    "                BertTokenizer(tokenizer_args), # convert input_text into input_ids, attention_masks, token_type_ids\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        pipeline.add_post_processing(\n",
    "            [\n",
    "                (BertTokenizerQADecoder(tokenizer_args), # decode the input_ids to text\n",
    "                [utils.IoMapEntry(\"BertTokenizer\", producer_idx=0, consumer_idx=2)]) # input_ids\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    new_model = pipeline.run(model)\n",
    "    onnx.save_model(new_model, output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = str(quantized_model).replace(\".onnx\", \"_with_pre_post_processing.onnx\")\n",
    "add_pre_post_processing(quantized_model, output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test output ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions import get_library_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_onnx_model(model_path: str):\n",
    "    \n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "    # Note: register the custom operators for the image decode/encode pre/post processing provided by onnxruntime-extensions\n",
    "    # with onnxruntime. if we do not do this we'll get an error on model load about the operators not being found.\n",
    "    ortext_lib_path = get_library_path()\n",
    "    so.register_custom_ops_library(ortext_lib_path)\n",
    "    inference_session = ort.InferenceSession(model_path, so)\n",
    "    \n",
    "\n",
    "    test_context = \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"\n",
    "    test_question = \"What day was the game played on?\"\n",
    "    outputs = inference_session.run(['text'], {'input_text': [[test_question, test_context]]})\n",
    "    output_answer = outputs[0][0]\n",
    "    print(\"Answer:  \" + output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  february 7, 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:33:10.145313 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '_ppp8_i64_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 12:33:10.204589 [W:onnxruntime:, unsqueeze_elimination.cc:20 Apply] UnsqueezeElimination cannot remove node post_process_7\n"
     ]
    }
   ],
   "source": [
    "test_onnx_model(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build and run inference with the output model in a mobile application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- iOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
