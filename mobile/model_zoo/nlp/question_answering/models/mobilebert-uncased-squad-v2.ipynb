{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Runtime Question Answering with MobileBert model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the end-to-end steps to obtain from a HuggingFace model, convert to ONNX format and then add pre/post processing steps to the ONNX model using onnxruntime-extensions library. And apply directly in a sample mobile android/ios application if applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to pip install `onnxruntime onnx onnxruntime_extensions transformers` as the necessary libraries.\n",
    "\n",
    "```\n",
    "    pip install onnx\n",
    "```\n",
    "```\n",
    "    pip install onnxruntime\n",
    "```\n",
    "```\n",
    "    pip install onnxruntime_extensions\n",
    "```\n",
    "```\n",
    "    pip install transformers\n",
    "```\n",
    "\n",
    "To work with Python in Jupyter Notebooks, you must activate an [Anaconda](https://www.anaconda.com/) environment or another Python environment in which you've installed the [Jupyter package](https://pypi.org/project/jupyter/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Prepare ONNX Model from HuggingFace MobileBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onnx_model_from_huggingface(hf_model_name, onnx_model_path):\n",
    "    \"\"\"\n",
    "        Load the model from huggingface and export it to onnx\n",
    "    \"\"\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(hf_model_name)\n",
    "    model = transformers.MobileBertForQuestionAnswering.from_pretrained(hf_model_name)\n",
    "    \n",
    "    model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=\"question-answering\")\n",
    "    onnx_config = model_onnx_config(model.config)\n",
    "    \n",
    "    # Export the hf model to onnx\n",
    "    onnx_inputs, onnx_outputs = transformers.onnx.export(tokenizer, # pretrained generic tokenizer class for the model\n",
    "                                                         model, # pretrained hf model\n",
    "                                                         onnx_config, # onnx configurations which includes input/output names/types info\n",
    "                                                         16, # opset_version - the ONNX version to export the model to\n",
    "                                                         onnx_model_path) # where to save the exported onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ONNX model from huggingface model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/models/mobilebert/modeling_mobilebert.py:547: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(1000),\n",
      "/usr/local/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.11/site-packages/torch/onnx/utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.11/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = Path('mobilebert_uncased_squad_v2.onnx')\n",
    "if not onnx_model_path.exists():\n",
    "    print(\"Creating ONNX model from huggingface model...\")\n",
    "    create_onnx_model_from_huggingface('csarron/mobilebert-uncased-squad-v2', onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output ONNX model is exported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert onnx_model_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model_path: Path):\n",
    "    \"\"\"\n",
    "        Quantize the model, so that it can be run on mobile devices with smaller memory footprint\n",
    "    \"\"\"\n",
    "    quantized_model_path = model_path.with_name(model_path.stem+\"_quant\").with_suffix(model_path.suffix)\n",
    "    quantize_dynamic(model_path, quantized_model_path, weight_type=QuantType.QInt8)\n",
    "    model_path.unlink()\n",
    "    return quantized_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.11/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.12/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.12/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.13/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.13/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.14/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.14/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.15/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.15/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.16/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.16/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.17/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.17/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.18/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.18/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.19/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.19/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.20/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.20/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.21/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.21/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.22/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.22/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.23/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/mobilebert/encoder/layer.23/attention/self/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Add pre and post processing steps to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions.tools.pre_post_processing import *\n",
    "from onnxruntime_extensions.tools import add_pre_post_processing_to_model as add_ppp\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pre_post_processing(input_model_path: Path, output_model_path: str, model_name: str = \"csarron/mobilebert-uncased-squad-v2\"):\n",
    "    \"\"\"\n",
    "    Add pre and post processing to the model, for tokenization and post processing\n",
    "    \"\"\"\n",
    "    onnx_opset = 16\n",
    "    model = onnx.load(str(input_model_path.resolve(strict=True)))\n",
    "    inputs = [create_named_value(\"input_text\", onnx.TensorProto.STRING, [1, \"num_sentences\"])]  # Fix the batch size to be 1\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    @contextmanager\n",
    "    def temp_vocab_file():\n",
    "        vocab_file = input_model_path.parent/ \"vocab.txt\"\n",
    "        yield vocab_file\n",
    "\n",
    "    with temp_vocab_file() as vocab_file:\n",
    "        import json\n",
    "        with open(str(vocab_file), 'w') as f:\n",
    "            f.write(json.dumps(tokenizer.vocab))\n",
    "\n",
    "        pipeline = PrePostProcessor(inputs, onnx_opset)\n",
    "        \n",
    "        tokenizer_args = TokenizerParam(\n",
    "            vocab_or_file=vocab_file,\n",
    "            do_lower_case=True,\n",
    "            tweaked_bos_id=0,\n",
    "            is_sentence_pair=True,\n",
    "        )\n",
    "        \n",
    "        pipeline.add_pre_processing(\n",
    "            [\n",
    "                BertTokenizer(tokenizer_args), # convert input_text into input_ids, attention_masks, token_type_ids\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        pipeline.add_post_processing(\n",
    "            [\n",
    "                (BertTokenizerQADecoder(tokenizer_args), # decode the input_ids to text\n",
    "                [utils.IoMapEntry(\"BertTokenizer\", producer_idx=0, consumer_idx=2)]) # input_ids\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    new_model = pipeline.run(model)\n",
    "    onnx.save_model(new_model, output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = str(quantized_model).replace(\".onnx\", \"_with_pre_post_processing.onnx\")\n",
    "add_pre_post_processing(quantized_model, output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test output ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime_extensions import get_library_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_onnx_model(model_path: str):\n",
    "    \n",
    "    so = ort.SessionOptions()\n",
    "    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "    # Note: register the custom operators for the image decode/encode pre/post processing provided by onnxruntime-extensions\n",
    "    # with onnxruntime. if we do not do this we'll get an error on model load about the operators not being found.\n",
    "    ortext_lib_path = get_library_path()\n",
    "    so.register_custom_ops_library(ortext_lib_path)\n",
    "    inference_session = ort.InferenceSession(model_path, so)\n",
    "    \n",
    "    test_context = \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"\n",
    "    test_question = \"What day was the game played on?\"\n",
    "    outputs = inference_session.run(['text'], {'input_text': [[test_question, test_context]]})\n",
    "    output_answer = outputs[0][0]\n",
    "    print(\"Answer:  \" + output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  february 7, 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 15:12:57.480080 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '_ppp10_i64_0'. It is not used by any node and should be removed from the model.\n",
      "2023-10-12 15:12:57.526583 [W:onnxruntime:, unsqueeze_elimination.cc:20 Apply] UnsqueezeElimination cannot remove node post_process_7\n"
     ]
    }
   ],
   "source": [
    "test_onnx_model(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build and run inference with the output model in a mobile application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plug in the generated onnx model and run inference easily in a mobile application (Android/iOS) with the pre/post processing support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Android**\n",
    "\n",
    "    **[ORT Question Answering Android Sample application](mobile/examples/question_answering/android)**\n",
    "\n",
    "    **Note:** You can skip the step about `preparing model` and place the above generated model under `mobile\\examples\\question_answering\\android\\app\\src\\main\\res\\raw` directory.\n",
    "\n",
    "    See more information about general prerequisites to build and run an Android application in [README.md/Requirements](mobile/examples/question_answering/android/README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for initalizing ort session and register ort extensions for pre/post processing support:\n",
    "```kotlin\n",
    "\n",
    "   // Initialize Ort Session and register the onnxruntime extensions package that contains the custom operators.\n",
    "    val sessionOptions: OrtSession.SessionOptions = OrtSession.SessionOptions()\n",
    "    if (ep.contains(\"CPU\")){\n",
    "    } else if (ep.contains(\"NNAPI\")) {\n",
    "        sessionOptions.addNnapi()\n",
    "    } else if (ep.contains(\"XNNAPCK\")) {\n",
    "        val po = mapOf<String, String>()\n",
    "        sessionOptions.addXnnpack(po)\n",
    "    }\n",
    "    sessionOptions.setSessionLogLevel(OrtLoggingLevel.ORT_LOGGING_LEVEL_VERBOSE)\n",
    "    sessionOptions.setSessionLogVerbosityLevel(0)\n",
    "    sessionOptions.registerCustomOpLibrary(OrtxPackage.getLibraryPath())\n",
    "    ortSession = ortEnv.createSession(readModel(), sessionOptions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for performing Question Answering task with MobileBERT model in an Android app:\n",
    "```kotlin\n",
    "    // Step 1: Prepare (question, article) pair as the input text\n",
    "    val article = article_seq.toString()\n",
    "    val question = qustion_seq.toString()\n",
    "\n",
    "    // Step 2: Create input tensor\n",
    "    val shape = longArrayOf(1, 2)\n",
    "    val inputTensor = OnnxTensor.createTensor(ortEnv, arrayOf(question, article), shape)\n",
    "\n",
    "    inputTensor.use {\n",
    "        // Step 3: Call Ort InferenceSession Run\n",
    "        val output = ortSession.run(Collections.singletonMap(\"input_text\", inputTensor))\n",
    "\n",
    "        // Step 4: Analyze the inference result\n",
    "        output.use {\n",
    "            val rawOutput = (output?.get(0)?.value) as Array<String>\n",
    "            outputAnswer = rawOutput[0]\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **iOS**\n",
    "\n",
    "    **[ORT Question Answering iOS Sample application](mobile/examples/question_answering/iOS)**\n",
    "\n",
    "    **Note:** You can skip the step about `prepare model` and place the above generated model under `mobile/examples/question_answering/ios/ORTQuestionAnswering/ORTQuestionAnswering`. Adding that as an bundle asset to your iOS app.\n",
    "\n",
    "    See more information about general prerequisites to build and run an iOS application in [README.md/Requirements](mobile/examples/question_answering/iOS/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code snippet for performing Question Answering task in an iOS app:\n",
    "```c\n",
    "    // Step 1: Register custom ops\n",
    "    const auto ort_log_level = ORT_LOGGING_LEVEL_INFO;\n",
    "    auto ort_env = Ort::Env(ort_log_level, \"ORTQuestionAnswering\");\n",
    "    auto session_options = Ort::SessionOptions(); \n",
    "    if (RegisterCustomOps(session_options, OrtGetApiBase()) != nullptr) {\n",
    "        throw std::runtime_error(\"RegisterCustomOps failed\");\n",
    "    }\n",
    "            \n",
    "    // Step 2: Load model   \n",
    "    NSString *model_path = [NSBundle.mainBundle pathForResource:@\"mobilebert_uncased_squad_v2_quant_with_pre_post_processing\"\n",
    "                                                                ofType:@\"onnx\"];\n",
    "    if (model_path == nullptr) {\n",
    "        throw std::runtime_error(\"Failed to get model path\");\n",
    "    }\n",
    "            \n",
    "    // Step 2: Create Ort Inference Session    \n",
    "    auto sess = Ort::Session(ort_env, [model_path UTF8String], session_options);\n",
    "            \n",
    "    // Step 3: Prepare input tensors and input/output names        \n",
    "    std::vector<int64_t> input_dims{1, 2};\n",
    "    Ort::AllocatorWithDefaultOptions ortAllocator;\n",
    "    auto input_tensor = Ort::Value::CreateTensor(ortAllocator, input_dims.data(), input_dims.size(), ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING);\n",
    "\n",
    "    std::vector<std::string> question_article_vec;\n",
    "    question_article_vec.reserve(2);\n",
    "    question_article_vec.push_back([input_user_question UTF8String]);\n",
    "    question_article_vec.push_back([input_article UTF8String]);\n",
    "    std::vector<const char*> p_str;\n",
    "    for (const auto& s : question_article_vec) {\n",
    "        p_str.push_back(s.c_str());\n",
    "    }\n",
    "    input_tensor.FillStringTensor(p_str.data(), p_str.size());\n",
    "            \n",
    "    // Step 4: Call inference session run\n",
    "    constexpr auto input_names = std::array{\"input_text\"};\n",
    "    constexpr auto output_names = std::array{\"text\"};\n",
    "    const auto outputs = sess.Run(Ort::RunOptions(), input_names.data(),\n",
    "                                &input_tensor, 1, output_names.data(), 1);\n",
    "                    \n",
    "    // Step 5: Analyze model outputs\n",
    "    if (outputs.size() != 1) {\n",
    "        throw std::runtime_error(\"Unexpected number of outputs\");\n",
    "    }        \n",
    "    const auto &output_tensor = outputs.front();\n",
    "    const std::string* output_string_raw = output_tensor.GetTensorData<std::string>();\n",
    "    output_text = [NSString stringWithCString:output_string_raw->c_str() encoding:NSUTF8StringEncoding];\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
